<?xml version="1.0" encoding="UTF-8"?>
<job>
  <name>orange_book_job</name>
  <description/>
  <extended_description/>
  <job_version/>
  <job_status>0</job_status>
  <directory>/</directory>
  <created_user>-</created_user>
  <created_date>2022/01/17 17:39:02.118</created_date>
  <modified_user>-</modified_user>
  <modified_date>2022/01/17 17:39:02.118</modified_date>
  <parameters>
    <parameter>
      <name>AWS_DEFAULT_REGION</name>
      <default_value>${AWS_DEFAULT_REGION}</default_value>
      <description/>
    </parameter>
    <parameter>
      <name>AWS_S3_ACCESS_KEY</name>
      <default_value>${AWS_S3_ACCESS_KEY}</default_value>
      <description/>
    </parameter>
    <parameter>
      <name>AWS_S3_BUCKET_NAME</name>
      <default_value>${AWS_DEFAULT_REGION}</default_value>
      <description/>
    </parameter>
    <parameter>
      <name>AWS_S3_SECRET_KEY</name>
      <default_value>${AWS_S3_SECRET_KEY}</default_value>
      <description/>
    </parameter>
    <parameter>
      <name>FAERSDBSTATS_REPO_LOCATION</name>
      <default_value>/home/pentaho-secondary/projects-brb265/faers/faersdbstats</default_value>
      <description/>
    </parameter>
  </parameters>
  <connection>
    <name>cem_sandbox_log</name>
    <server>napdi-pv-faers.cvafeuxzoav4.us-west-2.rds.amazonaws.com</server>
    <type>POSTGRESQL</type>
    <access>Native</access>
    <database>cem_sandbox</database>
    <port>5432</port>
    <username>brb265</username>
    <password>Encrypted 2be98afc86aa7f2e4bf298747f7bc9dcd</password>
    <servername/>
    <data_tablespace/>
    <index_tablespace/>
    <attributes>
      <attribute>
        <code>FORCE_IDENTIFIERS_TO_LOWERCASE</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>FORCE_IDENTIFIERS_TO_UPPERCASE</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>IS_CLUSTERED</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>PORT_NUMBER</code>
        <attribute>5432</attribute>
      </attribute>
      <attribute>
        <code>PRESERVE_RESERVED_WORD_CASE</code>
        <attribute>Y</attribute>
      </attribute>
      <attribute>
        <code>QUOTE_ALL_FIELDS</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>SUPPORTS_BOOLEAN_DATA_TYPE</code>
        <attribute>Y</attribute>
      </attribute>
      <attribute>
        <code>SUPPORTS_TIMESTAMP_DATA_TYPE</code>
        <attribute>Y</attribute>
      </attribute>
      <attribute>
        <code>USE_POOLING</code>
        <attribute>N</attribute>
      </attribute>
    </attributes>
  </connection>
  <slaveservers>
    </slaveservers>
  <job-log-table>
    <connection>cem_sandbox_log</connection>
    <schema>public</schema>
    <table>pdi_logging</table>
    <size_limit_lines>5000</size_limit_lines>
    <interval>1s</interval>
    <timeout_days>3.0</timeout_days>
    <field>
      <id>ID_JOB</id>
      <enabled>Y</enabled>
      <name>ID_JOB</name>
    </field>
    <field>
      <id>CHANNEL_ID</id>
      <enabled>Y</enabled>
      <name>CHANNEL_ID</name>
    </field>
    <field>
      <id>JOBNAME</id>
      <enabled>Y</enabled>
      <name>JOBNAME</name>
    </field>
    <field>
      <id>STATUS</id>
      <enabled>Y</enabled>
      <name>STATUS</name>
    </field>
    <field>
      <id>LINES_READ</id>
      <enabled>Y</enabled>
      <name>LINES_READ</name>
    </field>
    <field>
      <id>LINES_WRITTEN</id>
      <enabled>Y</enabled>
      <name>LINES_WRITTEN</name>
    </field>
    <field>
      <id>LINES_UPDATED</id>
      <enabled>Y</enabled>
      <name>LINES_UPDATED</name>
    </field>
    <field>
      <id>LINES_INPUT</id>
      <enabled>Y</enabled>
      <name>LINES_INPUT</name>
    </field>
    <field>
      <id>LINES_OUTPUT</id>
      <enabled>Y</enabled>
      <name>LINES_OUTPUT</name>
    </field>
    <field>
      <id>LINES_REJECTED</id>
      <enabled>Y</enabled>
      <name>LINES_REJECTED</name>
    </field>
    <field>
      <id>ERRORS</id>
      <enabled>Y</enabled>
      <name>ERRORS</name>
    </field>
    <field>
      <id>STARTDATE</id>
      <enabled>Y</enabled>
      <name>STARTDATE</name>
    </field>
    <field>
      <id>ENDDATE</id>
      <enabled>Y</enabled>
      <name>ENDDATE</name>
    </field>
    <field>
      <id>LOGDATE</id>
      <enabled>Y</enabled>
      <name>LOGDATE</name>
    </field>
    <field>
      <id>DEPDATE</id>
      <enabled>Y</enabled>
      <name>DEPDATE</name>
    </field>
    <field>
      <id>REPLAYDATE</id>
      <enabled>Y</enabled>
      <name>REPLAYDATE</name>
    </field>
    <field>
      <id>LOG_FIELD</id>
      <enabled>Y</enabled>
      <name>LOG_FIELD</name>
    </field>
    <field>
      <id>EXECUTING_SERVER</id>
      <enabled>N</enabled>
      <name>EXECUTING_SERVER</name>
    </field>
    <field>
      <id>EXECUTING_USER</id>
      <enabled>N</enabled>
      <name>EXECUTING_USER</name>
    </field>
    <field>
      <id>START_JOB_ENTRY</id>
      <enabled>N</enabled>
      <name>START_JOB_ENTRY</name>
    </field>
    <field>
      <id>CLIENT</id>
      <enabled>N</enabled>
      <name>CLIENT</name>
    </field>
  </job-log-table>
  <jobentry-log-table>
    <connection>cem_sandbox_log</connection>
    <schema>public</schema>
    <table>pdi_logging</table>
    <timeout_days>3.0</timeout_days>
    <field>
      <id>ID_BATCH</id>
      <enabled>Y</enabled>
      <name>ID_BATCH</name>
    </field>
    <field>
      <id>CHANNEL_ID</id>
      <enabled>Y</enabled>
      <name>CHANNEL_ID</name>
    </field>
    <field>
      <id>LOG_DATE</id>
      <enabled>Y</enabled>
      <name>LOG_DATE</name>
    </field>
    <field>
      <id>JOBNAME</id>
      <enabled>Y</enabled>
      <name>TRANSNAME</name>
    </field>
    <field>
      <id>JOBENTRYNAME</id>
      <enabled>Y</enabled>
      <name>STEPNAME</name>
    </field>
    <field>
      <id>LINES_READ</id>
      <enabled>Y</enabled>
      <name>LINES_READ</name>
    </field>
    <field>
      <id>LINES_WRITTEN</id>
      <enabled>Y</enabled>
      <name>LINES_WRITTEN</name>
    </field>
    <field>
      <id>LINES_UPDATED</id>
      <enabled>Y</enabled>
      <name>LINES_UPDATED</name>
    </field>
    <field>
      <id>LINES_INPUT</id>
      <enabled>Y</enabled>
      <name>LINES_INPUT</name>
    </field>
    <field>
      <id>LINES_OUTPUT</id>
      <enabled>Y</enabled>
      <name>LINES_OUTPUT</name>
    </field>
    <field>
      <id>LINES_REJECTED</id>
      <enabled>Y</enabled>
      <name>LINES_REJECTED</name>
    </field>
    <field>
      <id>ERRORS</id>
      <enabled>Y</enabled>
      <name>ERRORS</name>
    </field>
    <field>
      <id>RESULT</id>
      <enabled>Y</enabled>
      <name>RESULT</name>
    </field>
    <field>
      <id>NR_RESULT_ROWS</id>
      <enabled>Y</enabled>
      <name>NR_RESULT_ROWS</name>
    </field>
    <field>
      <id>NR_RESULT_FILES</id>
      <enabled>Y</enabled>
      <name>NR_RESULT_FILES</name>
    </field>
    <field>
      <id>LOG_FIELD</id>
      <enabled>N</enabled>
      <name>LOG_FIELD</name>
    </field>
    <field>
      <id>COPY_NR</id>
      <enabled>N</enabled>
      <name>COPY_NR</name>
    </field>
  </jobentry-log-table>
  <channel-log-table>
    <connection>cem_sandbox_log</connection>
    <schema>public</schema>
    <table>pdi_logging</table>
    <timeout_days>3.0</timeout_days>
    <field>
      <id>ID_BATCH</id>
      <enabled>Y</enabled>
      <name>ID_BATCH</name>
    </field>
    <field>
      <id>CHANNEL_ID</id>
      <enabled>Y</enabled>
      <name>CHANNEL_ID</name>
    </field>
    <field>
      <id>LOG_DATE</id>
      <enabled>Y</enabled>
      <name>LOG_DATE</name>
    </field>
    <field>
      <id>LOGGING_OBJECT_TYPE</id>
      <enabled>Y</enabled>
      <name>LOGGING_OBJECT_TYPE</name>
    </field>
    <field>
      <id>OBJECT_NAME</id>
      <enabled>Y</enabled>
      <name>OBJECT_NAME</name>
    </field>
    <field>
      <id>OBJECT_COPY</id>
      <enabled>Y</enabled>
      <name>OBJECT_COPY</name>
    </field>
    <field>
      <id>REPOSITORY_DIRECTORY</id>
      <enabled>Y</enabled>
      <name>REPOSITORY_DIRECTORY</name>
    </field>
    <field>
      <id>FILENAME</id>
      <enabled>Y</enabled>
      <name>FILENAME</name>
    </field>
    <field>
      <id>OBJECT_ID</id>
      <enabled>Y</enabled>
      <name>OBJECT_ID</name>
    </field>
    <field>
      <id>OBJECT_REVISION</id>
      <enabled>Y</enabled>
      <name>OBJECT_REVISION</name>
    </field>
    <field>
      <id>PARENT_CHANNEL_ID</id>
      <enabled>Y</enabled>
      <name>PARENT_CHANNEL_ID</name>
    </field>
    <field>
      <id>ROOT_CHANNEL_ID</id>
      <enabled>Y</enabled>
      <name>ROOT_CHANNEL_ID</name>
    </field>
  </channel-log-table>
  <pass_batchid>N</pass_batchid>
  <shared_objects_file/>
  <entries>
    <entry>
      <name>Unzip file</name>
      <description/>
      <type>UNZIP</type>
      <attributes/>
      <zipfilename/>
      <wildcard/>
      <wildcardexclude/>
      <targetdirectory/>
      <movetodirectory/>
      <afterunzip>0</afterunzip>
      <addfiletoresult>N</addfiletoresult>
      <isfromprevious>N</isfromprevious>
      <adddate>N</adddate>
      <addtime>N</addtime>
      <addOriginalTimestamp>N</addOriginalTimestamp>
      <SpecifyFormat>N</SpecifyFormat>
      <date_time_format/>
      <rootzip>N</rootzip>
      <createfolder>N</createfolder>
      <nr_limit>10</nr_limit>
      <wildcardSource/>
      <success_condition>success_if_no_errors</success_condition>
      <iffileexists>SKIP</iffileexists>
      <create_move_to_directory>N</create_move_to_directory>
      <setOriginalModificationDate>N</setOriginalModificationDate>
      <parallel>N</parallel>
      <draw>N</draw>
      <nr>0</nr>
      <xloc>208</xloc>
      <yloc>112</yloc>
      <attributes_kjc/>
    </entry>
    <entry>
      <name>Upload files to FTPS</name>
      <description/>
      <type>FTPS_PUT</type>
      <attributes/>
      <servername/>
      <serverport>21</serverport>
      <username/>
      <password>Encrypted </password>
      <remoteDirectory/>
      <localDirectory/>
      <wildcard/>
      <binary>N</binary>
      <timeout>0</timeout>
      <remove>N</remove>
      <only_new>N</only_new>
      <active>N</active>
      <proxy_host/>
      <proxy_port/>
      <proxy_username/>
      <proxy_password/>
      <connection_type>FTP_CONNECTION</connection_type>
      <parallel>N</parallel>
      <draw>N</draw>
      <nr>0</nr>
      <xloc>112</xloc>
      <yloc>48</yloc>
      <attributes_kjc/>
    </entry>
    <entry>
      <name>Start</name>
      <description/>
      <type>SPECIAL</type>
      <attributes/>
      <start>Y</start>
      <dummy>N</dummy>
      <repeat>N</repeat>
      <schedulerType>0</schedulerType>
      <intervalSeconds>0</intervalSeconds>
      <intervalMinutes>60</intervalMinutes>
      <hour>12</hour>
      <minutes>0</minutes>
      <weekDay>1</weekDay>
      <DayOfMonth>1</DayOfMonth>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>128</xloc>
      <yloc>32</yloc>
      <attributes_kjc/>
    </entry>
    <entry>
      <name>orange_download.sh</name>
      <description/>
      <type>SHELL</type>
      <attributes/>
      <filename>${FAERSDBSTATS_REPO_LOCATION}/reference_and_mapping_data/orange_download.sh</filename>
      <work_directory/>
      <arg_from_previous>N</arg_from_previous>
      <exec_per_row>N</exec_per_row>
      <set_logfile>Y</set_logfile>
      <logfile>orange_book_lob</logfile>
      <set_append_logfile>Y</set_append_logfile>
      <logext>txt</logext>
      <add_date>Y</add_date>
      <add_time>Y</add_time>
      <insertScript>N</insertScript>
      <script>#!/bin/bash 

cd ${FAERSDBSTATS_REPO_LOCATION}

cd ..

echo pwd is
pwd

curl -f ${CEM_ORANGE_BOOK_DOWNLOAD_URL} -JLO &amp;&amp; echo "SUCCESS!" ||
    echo "It did not download double check your URL"

#works
#curl -f 'https://www.fda.gov/media/76860/download' -JLO &amp;&amp; echo "SUCCESS!" ||
#    echo "It did not download double check your URL"

filename=$(find . -maxdepth 1 -name "EOBZIP_*")

filename="${filename:2}"

echo pwd is
pwd
downloadlocation=$(pwd)

# figure out how to use zipinfo as a test &lt; shows it's downloaded and actually a zip
echo filename is $filename

zipinfo $filename

cd ${FAERSDBSTATS_REPO_LOCATION}/${CEM_DOWNLOAD_DATA_FOLDER}

mkdir -p "orange-book-data-files"

#unzip 
unzip $downloadlocation/$filename -d "orange-book-data-files"</script>
      <loglevel>Rowlevel</loglevel>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>128</xloc>
      <yloc>144</yloc>
      <attributes_kjc/>
    </entry>
    <entry>
      <name>Orange_Data_Files_Upload_To_S3</name>
      <description/>
      <type>SHELL</type>
      <attributes/>
      <filename/>
      <work_directory/>
      <arg_from_previous>N</arg_from_previous>
      <exec_per_row>N</exec_per_row>
      <set_logfile>N</set_logfile>
      <logfile/>
      <set_append_logfile>N</set_append_logfile>
      <logext/>
      <add_date>N</add_date>
      <add_time>N</add_time>
      <insertScript>Y</insertScript>
      <script>#!/bin/bash 

#create folder
#aws s3api put-object --bucket ${AWS_S3_BUCKET_NAME} --key orange-book-download-files/${CEM_DOWNLOAD_DATA_FOLDER}_setup

export "AWS_ACCESS_KEY_ID=${AWS_S3_ACCESS_KEY}"
#echo "AWS_S3_ACCESS_KEY = ${AWS_S3_ACCESS_KEY}"

export "AWS_SECRET_ACCESS_KEY=${AWS_S3_SECRET_KEY}"
#echo "AWS_S3_BUCKET_NAME=${AWS_S3_BUCKET_NAME}"

export "AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION}"

aws configure list

#echo "aws s3 cp ${BASE_FILE_DIR}/${CEM_DOWNLOAD_DATA_FOLDER}_setup/orange-book-data-files/ s3://napdi-cem-sandbox-files/${CEM_DOWNLOAD_DATA_FOLDER}_setup --recursive"
aws s3 cp ${BASE_FILE_DIR}/${CEM_DOWNLOAD_DATA_FOLDER}_setup/orange-book-data-files/ s3://napdi-cem-sandbox-files/${CEM_DOWNLOAD_DATA_FOLDER}_setup --recursive



</script>
      <loglevel>Basic</loglevel>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>128</xloc>
      <yloc>272</yloc>
      <attributes_kjc/>
    </entry>
    <entry>
      <name>orange_book_transform</name>
      <description/>
      <type>TRANS</type>
      <attributes/>
      <specification_method>filename</specification_method>
      <trans_object_id/>
      <filename>${Internal.Entry.Current.Directory}/orange_book_transform.ktr</filename>
      <transname/>
      <arg_from_previous>N</arg_from_previous>
      <params_from_previous>N</params_from_previous>
      <exec_per_row>N</exec_per_row>
      <clear_rows>N</clear_rows>
      <clear_files>N</clear_files>
      <set_logfile>Y</set_logfile>
      <logfile>${FAERSDBSTATS_REPO_LOCATION}/../logs/orange_book_transform</logfile>
      <logext>txt</logext>
      <add_date>Y</add_date>
      <add_time>Y</add_time>
      <loglevel>Rowlevel</loglevel>
      <cluster>N</cluster>
      <slave_server_name/>
      <set_append_logfile>Y</set_append_logfile>
      <wait_until_finished>Y</wait_until_finished>
      <follow_abort_remote>N</follow_abort_remote>
      <create_parent_folder>Y</create_parent_folder>
      <logging_remote_work>N</logging_remote_work>
      <run_configuration>Pentaho local</run_configuration>
      <suppress_result_data>N</suppress_result_data>
      <parameters>
        <pass_all_parameters>Y</pass_all_parameters>
      </parameters>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>128</xloc>
      <yloc>384</yloc>
      <attributes_kjc/>
    </entry>
    <entry>
      <name>Set job variables</name>
      <description/>
      <type>SET_VARIABLES</type>
      <attributes/>
      <replacevars>Y</replacevars>
      <filename>${Internal.Entry.Current.Directory}/../../faers.config</filename>
      <file_variable_type>JVM</file_variable_type>
      <fields>
        <field>
          <variable_name>FILE_DIR</variable_name>
          <variable_value>${BASE_FILE_DIR}</variable_value>
          <variable_type>JVM</variable_type>
        </field>
        <field>
          <variable_name>FAERSDBSTATS_REPO_LOCATION</variable_name>
          <variable_value>${FAERSDBSTATS_REPO_LOCATION}</variable_value>
          <variable_type>JVM</variable_type>
        </field>
        <field>
          <variable_name>AWS_S3_BUCKET_NAME</variable_name>
          <variable_value>${AWS_S3_BUCKET_NAME}</variable_value>
          <variable_type>JVM</variable_type>
        </field>
        <field>
          <variable_name>AWS_S3_ACCESS_KEY</variable_name>
          <variable_value>${AWS_S3_ACCESS_KEY}</variable_value>
          <variable_type>JVM</variable_type>
        </field>
        <field>
          <variable_name>AWS_S3_SECRET_KEY</variable_name>
          <variable_value>${AWS_S3_SECRET_KEY}</variable_value>
          <variable_type>JVM</variable_type>
        </field>
        <field>
          <variable_name>AWS_DEFAULT_REGION</variable_name>
          <variable_value>${AWS_DEFAULT_REGION}</variable_value>
          <variable_type>JVM</variable_type>
        </field>
      </fields>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>339</xloc>
      <yloc>121</yloc>
      <attributes_kjc/>
    </entry>
    <entry>
      <name>Shell</name>
      <description/>
      <type>SHELL</type>
      <attributes/>
      <filename/>
      <work_directory/>
      <arg_from_previous>N</arg_from_previous>
      <exec_per_row>N</exec_per_row>
      <set_logfile>N</set_logfile>
      <logfile/>
      <set_append_logfile>N</set_append_logfile>
      <logext/>
      <add_date>N</add_date>
      <add_time>N</add_time>
      <insertScript>N</insertScript>
      <script/>
      <loglevel>Basic</loglevel>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>1500</xloc>
      <yloc>484</yloc>
      <attributes_kjc/>
    </entry>
    <entry>
      <name>Shell 2</name>
      <description/>
      <type>SHELL</type>
      <attributes/>
      <filename/>
      <work_directory/>
      <arg_from_previous>N</arg_from_previous>
      <exec_per_row>N</exec_per_row>
      <set_logfile>N</set_logfile>
      <logfile/>
      <set_append_logfile>N</set_append_logfile>
      <logext/>
      <add_date>N</add_date>
      <add_time>N</add_time>
      <insertScript>N</insertScript>
      <script/>
      <loglevel>Basic</loglevel>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>1700</xloc>
      <yloc>484</yloc>
      <attributes_kjc/>
    </entry>
    <entry>
      <name>setup aws credentials</name>
      <description/>
      <type>SHELL</type>
      <attributes/>
      <filename/>
      <work_directory/>
      <arg_from_previous>N</arg_from_previous>
      <exec_per_row>N</exec_per_row>
      <set_logfile>N</set_logfile>
      <logfile/>
      <set_append_logfile>N</set_append_logfile>
      <logext/>
      <add_date>N</add_date>
      <add_time>N</add_time>
      <insertScript>Y</insertScript>
      <script>#echo "last minute aws credential check"
#export AWS_S3_BUCKET_NAME="${AWS_S3_BUCKET_NAME}"
#echo "AWS_S3_BUCKET_NAME=${AWS_S3_BUCKET_NAME}"

#export AWS_S3_ACCESS_KEY="${AWS_S3_ACCESS_KEY}"
#echo "AWS_S3_ACCESS_KEY = ${AWS_S3_ACCESS_KEY}"

#echo "AWS_S3_SECRET_KEY=${AWS_S3_SECRET_KEY}"
#echo "AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION}"

aws configure list</script>
      <loglevel>Basic</loglevel>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>304</xloc>
      <yloc>240</yloc>
      <attributes_kjc/>
    </entry>
  </entries>
  <hops>
    <hop>
      <from>orange_download.sh</from>
      <to>Orange_Data_Files_Upload_To_S3</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>Orange_Data_Files_Upload_To_S3</from>
      <to>orange_book_transform</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>Start</from>
      <to>orange_download.sh</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>N</enabled>
      <evaluation>Y</evaluation>
      <unconditional>Y</unconditional>
    </hop>
    <hop>
      <from>Start</from>
      <to>Set job variables</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>Y</unconditional>
    </hop>
    <hop>
      <from>Set job variables</from>
      <to>orange_download.sh</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>Shell</from>
      <to>Shell 2</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
  </hops>
  <notepads>
    <notepad>
      <note>This job...

~ orange_download.sh
    - Downloads data (products.txt) from ... https://www.fda.gov/drugs/drug-approvals-and-databases/orange-book-data-files

    - Unzips the file.

~ Orange_Data_Files_Upload_To_S3
    - Uploads the 3 files to aws s3 bucket in data folder

~orange_book_transform
    - puts products.txt data into nda database

[note: Set job variables only needed if run independently of Fears_meta]
[...also to resolve permissions issues on orange_download.sh cd to location and chmod +m orange_download.sh]

TODO learn why aws credentials won't stay in env between steps</note>
      <xloc>464</xloc>
      <yloc>16</yloc>
      <width>845</width>
      <heigth>299</heigth>
      <fontname>Ubuntu</fontname>
      <fontsize>11</fontsize>
      <fontbold>N</fontbold>
      <fontitalic>N</fontitalic>
      <fontcolorred>0</fontcolorred>
      <fontcolorgreen>0</fontcolorgreen>
      <fontcolorblue>0</fontcolorblue>
      <backgroundcolorred>255</backgroundcolorred>
      <backgroundcolorgreen>205</backgroundcolorgreen>
      <backgroundcolorblue>112</backgroundcolorblue>
      <bordercolorred>100</bordercolorred>
      <bordercolorgreen>100</bordercolorgreen>
      <bordercolorblue>100</bordercolorblue>
      <drawshadow>Y</drawshadow>
    </notepad>
  </notepads>
  <attributes>
    <group>
      <name>METASTORE.pentaho</name>
      <attribute>
        <key>Default Run Configuration</key>
        <value>{"namespace":"pentaho","id":"Default Run Configuration","name":"Default Run Configuration","description":"Defines a default run configuration","metaStoreName":null}</value>
      </attribute>
    </group>
    <group>
      <name>{"_":"Embedded MetaStore Elements","namespace":"pentaho","type":"Default Run Configuration"}</name>
      <attribute>
        <key>Pentaho local</key>
        <value>{"children":[{"children":[],"id":"server","value":null},{"children":[],"id":"clustered","value":"N"},{"children":[],"id":"name","value":"Pentaho local"},{"children":[],"id":"description","value":null},{"children":[],"id":"pentaho","value":"N"},{"children":[],"id":"readOnly","value":"Y"},{"children":[],"id":"sendResources","value":"N"},{"children":[],"id":"logRemoteExecutionLocally","value":"N"},{"children":[],"id":"remote","value":"N"},{"children":[],"id":"local","value":"Y"},{"children":[],"id":"showTransformations","value":"N"}],"id":"Pentaho local","value":null,"name":"Pentaho local","owner":null,"ownerPermissionsList":[]}</value>
      </attribute>
    </group>
  </attributes>
</job>
